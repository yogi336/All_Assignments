{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def make_directory(dirname):\n",
    "    current_path = os.getcwd()\n",
    "    path = os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} images\".format(index + 1,len(data['image_urls'])))\n",
    "        response = requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),\"wb\") as file: \n",
    "            file.write(response.content)\n",
    "            \n",
    "def save_data_to_csv(data,filename):\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(filename, mode=\"a\", encoding= 'utf-8-sig')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_images_url(driver):\n",
    "    images = driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")\n",
    "    brands = driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "    product_description = driver.find_elements_by_xpath(\"//div[@class='_2mylT6 _3Ockxk']/a[1]\")\n",
    "    prices = driver.find_elements_by_xpath(\"//div[@class='_1vC4OE']//div[@class=_3auQ3N]\")\n",
    "    \n",
    "    product_data = {}\n",
    "    product_data['image_urls']=[]\n",
    "    product_data['product_description']=[]\n",
    "    product_data['brands']=[]\n",
    "    product_data['prices']=[]\n",
    "    \n",
    "    for image in images:\n",
    "        source = image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "        \n",
    "    for brand in brands:\n",
    "        product_data['brands'].append(brand.text)\n",
    "        \n",
    "    for product_desc in product_description:\n",
    "        product_data['product_description'].append(product_desc.text)\n",
    "        \n",
    "    for price in prices:\n",
    "        product_data['prices'].append(price.text)\n",
    "    print(\"Returning scraped data\")\n",
    "    \n",
    "    return product_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "#from ipynb.fs.full.save_images import make_directory,save_images,save_data_to_csv\n",
    "#from scrap_images import scrap_images_url\n",
    "#from ipynb.fs.full.scrap_images import scrap_images_url\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH ='D:\\Desktop4\\DataFiles\\Web_scaping\\chromedriver.exe'\n",
    "\n",
    "driver = webdriver.Chrome(executable_path =DRIVER_PATH)\n",
    "\n",
    "current_page_url=driver.get('https://www.flipkart.com/clothing-and-accessories/bottomwear/jeans/men-jeans/pr?sid=clo%2Cvua%2Ck58%2Ci51&otracker=categorytree&otracker=nmenu_sub_Men_0_Jeans&page=4')\n",
    "\n",
    "DIRNAME = 'men_leans'\n",
    "make_directory(DIRNAME)\n",
    "\n",
    "start_page =1\n",
    "total_page =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning scraped data\n",
      "product_details len\n",
      "Srcaping Page 1 of 5 pages\n",
      "The current page scrapped is 4\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scrapped of page 1 done\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-92dd3eb8f525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scrapped of page {0} done\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msave_data_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduct_details\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'men_leans.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Moving to the next page\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cc0cbc596e71>\u001b[0m in \u001b[0;36msave_data_to_csv\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_data_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    433\u001b[0m             )\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         ]\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "for page in range(start_page,total_page+1):\n",
    "    try:\n",
    "        product_details = scrap_images_url(driver=driver)\n",
    "        print(\"product_details len\")\n",
    "        print(\"Srcaping Page {0} of {1} pages\".format(page,total_page))\n",
    "        \n",
    "        page_value = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The current page scrapped is {}\".format(page_value))\n",
    "        \n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scrapped of page {0} done\".format(page))\n",
    "        \n",
    "        save_data_to_csv(data=product_details,filename='men_leans.csv')\n",
    "        \n",
    "        print(\"Moving to the next page\")\n",
    "        button_type = driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        \n",
    "        if button_type ==\"Next\":\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "            \n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()         \n",
    "            \n",
    "            \n",
    "        new_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the new page is {}\".format(new_page))\n",
    "        \n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing an exception\")\n",
    "        exp_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        ptint(\"The page value at the time of exception is {}\".format(exp_page))\n",
    "            \n",
    "        value = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\")\n",
    "        link = value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "            \n",
    "        product_details = scrap_images_url(driver=driver)\n",
    "        print(\"Scrapping Page {0} of {1} pages\".format(page,total_pages))\n",
    "            \n",
    "        page_value = driver,find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The Current page scrapped is {}\".format(page_value))\n",
    "            \n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scrapping of page {0} done\".format(page))\n",
    "            \n",
    "        save_data_to_csv(data=product_details,filename='men_leans.csv')\n",
    "            \n",
    "        print(\"Moving to the next page\")\n",
    "            \n",
    "        button_type = driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "            \n",
    "            \n",
    "        if button_type ==\"Next\":\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()         \n",
    "\n",
    "\n",
    "        new_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the new page is {}\".format(new_page))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
